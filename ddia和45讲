==============================================================================================================================

02

与查询流程不一样的是，更新流程还涉及两个重要的日志模块，它们正是我们今天要讨论的主角：redo log（重做日志）和 binlog（归档日志）
redo log是InnoDB引擎特有的日志
binlog只用于归档，属于server层，只依赖ta是没有办法做到crash-safe的

不同点
- redo log是InnoDB引擎独有的，binlog是mysql server层实现的，所有引擎都可以使用
- redo log是物理层，binlog是逻辑日志，记录的是这个语句的原始逻辑
- redo log类似ringbuffer的结构，由checkpoint和write pos组成，写完一个分片之后会覆盖
  binlog为WAL模式，会追加写，不会覆盖

redo log 和 binlog 都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致

Binlog有两种模式，statement 格式的话是记sql语句， row格式会记录行的内容，记两条，更新前和更新后都有

关键问题：2PC的前两步成功，第三部失败怎么办？即：步骤1.InnoDB执行事务prepare操作成功；步骤2.Server写binlog成功；步骤第三部commit失败的情况。假设MySQL第三步前宕机，重启后MySQL首先扫描binlog文件，收集最近写入的xid【事务ID，redo log记录里也会有该信息】；然后会扫描redo log里有哪些未提交的事务【即prepare状态的事务】，如果事务写入binlog，那么执行commit；如果没有写入binlog【即事务ID在binlog中查找不到】，那么回滚该事务。这样就可以保证两者的一致性了。【官方文档：https://dev.mysql.com/doc/refman/8.0/en/binary-log.html】


高性能总会伴随各种buffer，而与高可用总是伴随各种日志


为该讲的内容总结了几个问题, 大家复习的时候可以先尝试回答这些问题检查自己的掌握程度:
   1. redo log的概念是什么? 为什么会存在.
   2. 什么是WAL(write-ahead log)机制, 好处是什么.
   3. redo log 为什么可以保证crash safe机制.
   4. binlog的概念是什么, 起到什么作用, 可以做crash safe吗? 
   5. binlog和redolog的不同点有哪些? 
   6. 物理一致性和逻辑一直性各应该怎么理解? 
   7. 执行器和innoDB在执行update语句时候的流程是什么样的?
   8. 如果数据库误操作, 如何执行数据恢复?
   9. 什么是两阶段提交, 为什么需要两阶段提交, 两阶段提交怎么保证数据库中两份日志间的逻辑一致性(什么叫逻辑一致性)?
  10. 如果不是两阶段提交, 先写redo log和先写bin log两种情况各会遇到什么问题?

1. redo log是重做日志。主要用于MySQL异常重启后的一种数据恢复手段，确保了数据的一致性。归根到底是MySQL为了实现WAL机制的一种手段。因为MySQL进行更新操作，为了能够快速响应，所以采用了异步写回磁盘的技术，写入内存后就返回。但是会存在crash后内存数据丢失的隐患，而redo log具备crash safe能力。
2. WAL机制是写前日志，也就是MySQL更新操作后在真正把数据写入到磁盘前先记录日志。好处是不用每一次操作都实时把数据写盘，就算crash后也可以通过redo log重放恢复，所以能够实现快速响应SQL语句。
3. 因为redo log是每次更新操作完成后，就一定会写入的，如果写入失败，这说明此次操作失败，事务也不可能提交。redo log内部结构是基于页的，记录了这个页的字段值变化，只要crash后读取redo log进行重放就可以恢复数据。（因为redo log是循环写的，如果满了InnoDB就会执行真正写盘）
4. bin log是归档日志，属于MySQL Server层的日志。可以起到全量备份的作用。当需要恢复数据时，可以取出某个时间范围内的bin log进行重放恢复。但是bin log不可以做crash safe，因为crash之前，bin log可能没有写入完全MySQL就挂了。所以需要配合redo log才可以进行crash safe。
5. bin log是Server层，追加写，不会覆盖，记录了逻辑变化，是逻辑日志。redo log是存储引擎层，是InnoDB特有的。循环写，满了就覆盖从头写，记录的是基于页的物理变化，是物理日志，具备crash safe操作。
6. 前者是数据的一致性，后者是行为一致性。（不清楚）
7. 执行器在优化器选择了索引后，调用InnoDB读接口，读取要更新的行到内存中，执行SQL操作后，更新到内存，然后写redo log，写bin log，此时即为完成。后续InnoDB会在合适的时候把此次操作的结果写回到磁盘。
8. 数据库在某一天误操作，就可以找到距离误操作最近的时间节点前的bin log，重放到临时数据库里，然后选择当天误删的数据恢复到线上数据库。
9. 两阶段提交就是对于三步操作而言：1.prepare阶段 2. 写入bin log 3. commit
redo log在写入后，进入prepare状态，然后bin log写入后，进入commit状态，事务可以提交。
如果不用两阶段提交的话，可能会出现bin log写入之前，机器crash导致重启后redo log继续重放crash之前的操作，而当bin log后续需要作为备份恢复时，会出现数据不一致的情况。所以需要对redo log进行回滚。
如果是bin log commit之前crash，那么重启后，发现redo log是prepare状态且bin log完整（bin log写入成功后，redo log会有bin log的标记），就会自动commit，让存储引擎提交事务。
10.先写redo log，crash后bin log备份恢复时少了一次更新，与当前数据不一致。先写bin log，crash后，由于redo log没写入，事务无效，所以后续bin log备份恢复时，数据不一致。


==============================================================================================================================

03

隔离性和隔离级别

提到事务，你肯定会想到 ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），今天我们就来说说其中 I，也就是“隔离性”。
当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。
读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）

一条简单的更新语句，MySQL是如何加锁的？
https://zhuanlan.zhihu.com/p/94778920




Read Committed（RC）：针对当前读，RC隔离级别保证了对读取到的记录加锁（记录锁），存在幻读现象。
Repeatable Read（RR）：针对当前读，RR隔离级别保证对读取到的记录加锁（记录锁），同时保证对读取的范围加锁，新的满足查询条件的记录不能够插入（间隙锁），不存在幻读现象

在RR隔离级别下，id列上有非唯一索引，对于上述的SQL语句；首先，通过id索引定位到第一条满足条件的记录，给记录加上X锁，并且给Gap加上Gap锁，然后在主键聚簇索引上满足相同条件的记录加上X锁，然后返回；之后读取下一条记录重复进行。直至第一条出现不满足条件的记录，此时，不需要给记录加上X锁，但是需要给Gap加上Gap锁吗，最后返回结果

在MySQL/InnoDB中，所谓的读不加锁，并不适用于所有的情况，而是和隔离级别有关。在Serializable隔离级别下，所有的操作都会加锁


事务隔离级别                             脏读     不可重复读        幻读
读未提交（read-uncommitted）              是          是            是
不可重复读（read-committed）              否          是            是
可重复读（repeatable-read）               否          否            是
串行化（serializable）                   否           否            否



一般我们说可重复的效率相对的低（其实也还好，不会低多少），
主要还是因为可重复读的锁范围可能更大（有gap lock），锁时间更长（事务结束才释放），影响并发度


MVCC
MVCC，全称 Multi-Version Concurrency Control ，即多版本并发控制。MVCC 是一种并发控制的方法，一般在数据库管理系统中，实现对数据库的并发访问，在编程语言中实现事务内存

MVCC 模型在 MySQL 中的具体实现则是由 3 个隐式字段，undo 日志 ， Read View 等去完成的，具体可以看下面的 MVCC 实现原理

总之在 RC 隔离级别下，是每个快照读都会生成并获取最新的 Read View；而在 RR 隔离级别下，则是同一个事务中的第一个快照读才会创建 Read View, 之后的快照读获取的都是同一个 Read View

- DB_TRX_ID
6 byte，最近修改(修改/插入)事务 ID：记录创建这条记录/最后一次修改该记录的事务 ID
- DB_ROLL_PTR
7 byte，回滚指针，指向这条记录的上一个版本（存储于 rollback segment 里）
- DB_ROW_ID
6 byte，隐含的自增 ID（隐藏主键），如果数据表没有主键，InnoDB 会自动以DB_ROW_ID产生一个聚簇索引

一文搞懂Undo Log版本链与ReadView机制如何让事务读取到该读的数据
https://database.51cto.com/art/202101/641019.htm


【MySQL笔记】正确的理解MySQL的MVCC及实现原理
https://blog.csdn.net/SnailMann/article/details/94724197


数据库事务相关
https://blog.csdn.net/m2920440052/article/details/104989235

==============================================================================================================================

04

索引模型
- 索引组织表
- clustered index, secondary index
- 回表
- 页分裂 -> 页的利用率也会受影响 50%利用率降低
 

在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。又因为前面我们提到的，InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的

主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（clustered index）
非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（secondary index）

如果语句是 select * from T where k=5，即普通索引查询方式，则需要先搜索 k 索引树，得到 ID 的值为 500，再到 ID 索引树搜索一次。这个过程称为回表


B+树数据结构
- 通常用于数据库和操作系统的文件系统中。B+树的特点是能够保持数据稳定有序，其插入与修改拥有较稳定的对数时间复杂度。B+树元素自底向上插入，这与二叉树恰好相反

b+树详解
https://blog.csdn.net/qq_45814695/article/details/117171536

MySQL Innodb 数据页结构分析
https://www.cnblogs.com/bdsir/p/8745553.html


【MySQL】InnoDB行格式、数据页结构以及索引底层原理分析
https://blog.csdn.net/cy973071263/article/details/104512020

总结：
1.索引的作用：提高数据查询效率
2.常见索引模型：哈希表、有序数组、搜索树
3.哈希表：键 - 值(key - value)。
4.哈希思路：把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置
5.哈希冲突的处理办法：链表
6.哈希表适用场景：只有等值查询的场景
7.有序数组：按顺序存储。查询用二分法就可以快速查询，时间复杂度是：O(log(N))
8.有序数组查询效率高，更新效率低
9.有序数组的适用场景：静态存储引擎。
10.二叉搜索树：每个节点的左儿子小于父节点，父节点又小于右儿子
11.二叉搜索树：查询时间复杂度O(log(N))，更新时间复杂度O(log(N))
12.数据库存储大多不适用二叉树，因为树高过高，会适用N叉树
13.InnoDB中的索引模型：B+Tree
14.索引类型：主键索引、非主键索引
  主键索引的叶子节点存的是整行的数据(聚簇索引)，非主键索引的叶子节点内容是主键的值(二级索引)
15.主键索引和普通索引的区别：主键索引只要搜索ID这个B+Tree即可拿到数据。普通索引先搜索索引拿到主键值，再到主键索引树搜索一次(回表)
16.一个数据页满了，按照B+Tree算法，新增加一个数据页，叫做页分裂，会导致性能下降。空间利用率降低大概50%。当相邻的两个数据页利用率很低的时候会做数据页合并，合并的过程是分裂过程的逆过程。
17.从性能和存储空间方面考量，自增主键往往是更合理的选择。

思考题：
如果删除，新建主键索引，会同时去修改普通索引对应的主键索引，性能消耗比较大。
删除重建普通索引貌似影响不大，不过要注意在业务低谷期操作，避免影响业务。


1，  通过改变key值来调整
N叉树中非叶子节点存放的是索引信息，索引包含Key和Point指针。Point指针固定为6个字节，假如Key为10个字节，那么单个索引就是16个字节。如果B+树中页大小为16K，
那么一个页就可以存储1024个索引，此时N就等于1024。我们通过改变Key的大小，就可以改变N的值
2，  改变页的大小
页越大，一页存放的索引就越多，N就越大


==============================================================================================================================


05 索引2

覆盖索引-回表
最左前缀
索引下推-index condition pushdown

==============================================================================================================================


06 锁

全局锁，表锁，行锁
- 全局锁 Flush tables with read lock (FTWRL)
- 表锁分类 一种是表锁，一种是元数据锁（meta data lock，MDL)。

MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务

ALTER TABLE tbl_name NOWAIT add column ...
ALTER TABLE tbl_name WAIT N add column ...

online DDL
1 对表加锁(表此时只读)
2 复制原表物理结构，创建新中间表
3 修改中间表的物理结构
4 把原表数据导入中间表中，数据同步完后，锁定中间表，并删除原表
5 rename中间表为原表
6 刷新数据字典，并释放锁

DDL DML
- DML（Data Manipulation Language）数据操作语言-数据库的基本操作，SQL中处理数据等操作统称为数据操纵语言,简而言之就是实现了基本的“增删改查”操作。包括的关键字有：select、update、delete、insert、merge
- DDL（Data Definition Language）数据定义语言-用于定义和管理 SQL 数据库中的所有对象的语言，对数据库中的某些对象(例如，database,table)进行管理。包括的关键字有：create、alter、drop、truncate、comment、grant、revoke


==============================================================================================================================

07 

总结：
两阶段锁：在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放， 而是要等到事务结束时才释放。
建议：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放。
死锁：当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态。
解决方案：
1、通过参数 innodb_lock_wait_timeout 根据实际业务场景来设置超时时间，InnoDB引擎默认值是50s。
2、发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑（默认是开启状态）。
如何解决热点行更新导致的性能问题？
1、如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关闭掉。一般不建议采用
2、控制并发度，对应相同行的更新，在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。
3、将热更新的行数据拆分成逻辑上的多行来减少锁冲突，但是业务复杂度可能会大大提高。

innodb行级锁是通过锁索引记录实现的，如果更新的列没建索引是会锁住整个表的。


==============================================================================================================================

08 事务到底是隔离的还是不隔离的？

- 2个“视图”的概念
  1. view即虚拟表
  2. consistent read view 

read view 构成
在实现上， InnoDB 为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。
数组里面事务 ID 的最小值记为低水位，当前系统里面已经创建过的事务 ID 的最大值加 1 记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。


数据版本，对于一个事务视图来说，除了自己的更新总是可见以外
有三种情况
- 版本未提交，不可见；
- 版本已提交，但是是在视图创建后提交的，不可见；
- 版本已提交，而且是在视图创建前提交的，可见。


这里就用到了这样一条规则：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）
- 类似volatile的机制

当前读和快照读

读锁（S 锁，共享锁）
写锁（X 锁，排他锁）
mysql> select k from t where id=1 lock in share mode;
mysql> select k from t where id=1 for update;


事务的可重复读的能力是怎么实现的？
- 可重复读的核心就是一致性读（consistent read）
  而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。


Innodb 要保证这个规则：事务启动以前所有还没提交的事务，它都不可见。
- 但是只存一个已经提交事务的最大值是不够的。 因为存在一个问题，那些比最大值小的事务，之后也可能更新（就是你说的98这个事务）
- 所以事务启动的时候还要保存“现在正在执行的所有事物ID列表”，如果一个row trx_id在这列表中，也要不可见


事务ID是根据启动时间递增的，事务ID越大，启动越迟。但事务提交时间与ID（或者说启动时间）无关，事务ID小的，也可以提前提交
2. 例子：【99 100 101 102】 事务ID按启动时间递增，但是提交时间与ID无关，假设事务101已提交，其余未提交。
3. 考虑创建事务100一致性视图时，
创建一致性视图时，分为三部分：
绿色部分：事务创建前已提交的最大事务及当前事务 ：如已提交的101和当前事务100
黄色部分：活跃事务列表，即视图创建前尚未提交 如【99 102】
红色部分：未启动事务，如【103， 104】
    低水位是 活跃列表中未提交的ID 最小的事务，如 99（注意而非101的事务!!!），低于这个ID的事务都是已提交的
     高水位是103 尚未开始
4. 考虑创建事务100一致性视图时，其他row trx_id版本事务对事务100 是否可见：
（a）row trx_id < 99, 落在绿色部分，版本已提交，且在视图创建且提交， 是可见的（如95）
（b）row trx_id >= 103 落在红色，不可见
（c） row_trx_id >=99 且<=102. 落在黄色部分，则要判断row_trx_id 是否在活跃列表里，上例活跃列表为【99、 102】:
         row_trx_id = 99\102 在活跃列表，属于视图100创建前 未提交，不可见
         row_trx_id = 100\101 不在活跃列表，在绿色部分，属于视图创建前提交的，或者属于当前事务，可见


这篇理论知识很丰富,需要先总结下
1.innodb支持RC和RR隔离级别实现是用的一致性视图(consistent read view)

2.事务在启动时会拍一个快照,这个快照是基于整个库的.
基于整个库的意思就是说一个事务内,整个库的修改对于该事务都是不可见的(对于快照读的情况)
如果在事务内select t表,另外的事务执行了DDL t表,根据发生时间,要嘛锁住要嘛报错(参考第六章)

3.事务是如何实现的MVCC呢?
(1)每个事务都有一个事务ID,叫做transaction id(严格递增)
(2)事务在启动时,找到已提交的最大事务ID记为up_limit_id。
(3)事务在更新一条语句时,比如id=1改为了id=2.会把id=1和该行之前的row trx_id写到undo log里,
并且在数据页上把id的值改为2,并且把修改这条语句的transaction id记在该行行头
(4)再定一个规矩,一个事务要查看一条数据时,必须先用该事务的up_limit_id与该行的transaction id做比对,
如果up_limit_id>=transaction id,那么可以看.如果up_limit_id<transaction id,则只能去undo log里去取。去undo log查找数据的时候,也需要做比对,必须up_limit_id>transaction id,才返回数据

4.什么是当前读,由于当前读都是先读后写,只能读当前的值,所以为当前读.会更新事务内的up_limit_id为该事务的transaction id

5.为什么rr能实现可重复读而rc不能,分两种情况
(1)快照读的情况下,rr不能更新事务内的up_limit_id,
    而rc每次会把up_limit_id更新为快照读之前最新已提交事务的transaction id,则rc不能可重复读
(2)当前读的情况下,rr是利用record lock+gap lock来实现的,而rc没有gap lock,所以rc不能可重复读


上面说的“如果失败就重新起一个事务”，里面判断是否成功的标准是 affected_rows 是不是等于预期值

==============================================================================================================================


补充篇


数据库事务相关
https://blog.csdn.net/m2920440052/article/details/104989235

Undo log分为Insert和Update两种，delete可以看做是一种特殊的update，即在记录上修改删除标记，对MVCC有帮助的实质是update undo log

insert undo log
代表事务在insert新记录时产生的undo log, 只在事务回滚时需要，并且在事务提交后可以被立即丢弃

update undo log
事务在进行update或delete时产生的undo log; 不仅在事务回滚时需要，在快照读时也需要；所以不能随便删除，只有在快速读或事务回滚不涉及该日志时，对应的日志才会被purge线程统一清除


==============================================================================================================================

09 普通索引和唯一索引，应该怎么选择？

有点难，看的头疼

第二种情况是，这个记录要更新的目标页不在内存中。这时，InnoDB 的处理流程如下
- 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
- 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。


如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭 change buffer。而在其他情况下，change buffer 都能提升更新性能


==============================================================================================================================

10  MySQL为什么有时候会选错索引？

采样统计
基数cardinality，区分度
show index
force index

analyze table t之后重新explain就准了



==============================================================================================================================

11  怎么给字符串字段加索引

mysql> select count(distinct email) as L from SUser;

==============================================================================================================================

ddia读书笔记

http://ddia.vonng.com/


第一章

reliability 可靠性
scalability 可伸缩性
maintainability 可维护性

fault-tolerence 容错
resilient 韧性

MTTF, mean time to failure  平均无故障时间


服务级别目标（SLO, service level objectives） 和 服务级别协议（SLA, service level agreements）


------------------------------------------------------------------

reliability 可靠性


------------------------------------------------------------------

scalability 可伸缩性

- 描述负载
负载可以用一些称为 负载参数（load parameters） 的数字来描述。参数的最佳选择取决于系统架构，它可能是每秒向 Web 服务器发出的请求、数据库中的读写比率、聊天室中同时活跃的用户数量、缓存命中率或其他东西。
除此之外，也许平均情况对你很重要，也许你的瓶颈是少数极端场景

扇出（fan-out）

- 描述性能
和负载，资源形成3角关联关系

scaling up 垂直伸缩
scaling out 横向伸缩

------------------------------------------------------------------

maintainability

可操作性（Operability）
简单性（Simplicity）
可演化性（Evolvability）


==============================================================================================================================

第二章 

声明式语言往往适合并行执行。现在，CPU 的速度通过核心（core）的增加变得更快，而不是以比以前更高的时钟速度运行【31】。
命令代码很难在多个核心和多个机器之间并行化，因为它指定了指令必须以特定顺序执行。声明式语言更具有并行执行的潜力，因为它们仅指定结果的模式

==============================================================================================================================

第三章

两大类存储引擎：日志结构（log-structured） 的存储引擎，以及 面向页面（page-oriented） 的存储引擎

SSTables 排序字符串表
LSM树 The Log-Structured Merge-Tree (LSM-Tree)
LSM 树的基本思想 —— 保存一系列在后台合并的 SSTables —— 简单而有效

memtables和sstables

LSM-tree 基本原理及应用
https://cloud.tencent.com/developer/news/340271

关于 LSM-tree 的内容和 LevelDB 的设计思想就介绍完了，主要包括写前日志 WAL，memtable，SStable 三个部分。逐层合并，逐层查找。
LSM-tree 的主要劣势是读写放大，关于读写放大可以通过一些其他策略去降低


还有一些不同的策略来确定 SSTables 被压缩和合并的顺序和时间。最常见的选择是 size-tiered 和 leveled compaction。
LevelDB 和 RocksDB 使用 leveled compaction（LevelDB 因此得名）
HBase 使用 size-tiered，Cassandra 同时支持这两种。
- 对于 sized-tiered，较新和较小的 SSTables 相继被合并到较旧的和较大的 SSTable 中。
- 对于 leveled compaction，key 范围被拆分到较小的 SSTables，而较旧的数据被移动到单独的层级（level），这使得压缩（compaction）能够更加增量地进行，并且使用较少的硬盘空间


B树和B+树

如果要更新 B 树中现有键的值，需要搜索包含该键的叶子页面，更改该页面中的值，并将该页面写回到硬盘（对该页面的任何引用都将保持有效）。
如果你想添加一个新的键，你需要找到其范围能包含新键的页面，并将其添加到该页面。如果页面中没有足够的可用空间容纳新键，则将其分成两个半满页面，并更新父页面以反映新的键范围分区


LSM优势
- LSM顺序写，通常能够比B树支持更高的写入吞吐量，部分原因是它们有时具有较低的写放大
- LSM不容易产生碎片，相对B树来说

LSM劣势
- 更高百分比的表现可能不稳定，因为概率上发生某个请求需要等待硬盘先完成昂贵的压缩操作; 数据库越大，压缩所需的硬盘带宽就越多
  如果写入吞吐量很高，并且压缩没有仔细配置好，有可能导致压缩跟不上写入速率
- 事务场景下b树通过锁实现更方便，b树的一个优点是每个键只存在于索引中的一个位置，而日志结构化的存储引擎可能在不同的段中有相同键的多个副本
  这个方面使得b树在想要提供强大的事务语义的数据库中很有吸引力：



其他索引结构
- 数据存储在索引中
  索引的行直接存储在索引中。这被称为聚集索引（clustered index）
  堆文件heap file存行数据，这样数据只存一份
  在 聚集索引（在索引中存储所有的行数据）和 非聚集索引（仅在索引中存储对数据的引用）之间的折衷被称为 覆盖索引（covering index） 或 包含列的索引（index with included columns）
- 全文搜索和模糊索引  
- 内存数据库
  提供多样性的数据结构


列式存储
- 聚合：数据立方体和物化视图
  materialized aggregates
  不同的是，物化视图是查询结果的实际副本，会被写入硬盘，而虚拟视图只是编写查询的一个捷径


总结
- OLTP和OLAP
- 在线库2个学派
  日志结构学派 
  就地更新学派 

==============================================================================================================================

第四章


本地函数调用是可预测的，并且成功或失败仅取决于受你控制的参数。网络请求是不可预知的：由于网络问题，请求或响应可能会丢失，或者远程计算机可能很慢或不可用，这些问题完全不在你的控制范围之内。网络问题是常见的，所以你必须预测他们，例如通过重试失败的请求。
本地函数调用要么返回结果，要么抛出异常，或者永远不返回（因为进入无限循环或进程崩溃）。网络请求有另一个可能的结果：由于超时，它可能会返回没有结果。在这种情况下，你根本不知道发生了什么：如果你没有得到来自远程服务的响应，你无法知道请求是否通过（我们将在 第八章 更详细地讨论这个问题）。
如果你重试失败的网络请求，可能会发生请求实际上正在通过，只有响应丢失。在这种情况下，重试将导致该操作被执行多次，除非你在协议中引入去重机制（幂等，即 idempotence）。本地函数调用没有这个问题。 （在 第十一章 更详细地讨论幂等性）
每次调用本地功能时，通常需要大致相同的时间来执行。网络请求比函数调用要慢得多，而且其延迟也是非常可变的：好的时候它可能会在不到一毫秒的时间内完成，但是当网络拥塞或者远程服务超载时，可能需要几秒钟的时间完成一样的东西。
调用本地函数时，可以高效地将引用（指针）传递给本地内存中的对象。当你发出一个网络请求时，所有这些参数都需要被编码成可以通过网络发送的一系列字节。如果参数是像数字或字符串这样的基本类型倒是没关系，但是对于较大的对象很快就会变成问题。
客户端和服务可以用不同的编程语言实现，所以 RPC 框架必须将数据类型从一种语言翻译成另一种语言。这可能会捅出大篓子，因为不是所有的语言都具有相同的类型 —— 例如回想一下 JavaScript 的数字大于 2^{53}2 
53
  的问题（请参阅 “JSON、XML 和二进制变体”）。用单一语言编写的单个进程中不存在此问题。



==============================================================================================================================


第二部分 分布式数据

复制和分区 Replications and Partitioning

第五章 复制

1. 领导者和追随者

基于语句的复制
- 任何调用 非确定性函数（nondeterministic） 的语句，可能会在每个副本上生成不同的值。例如，使用 NOW() 获取当前日期时间，或使用 RAND() 获取一个随机数。
- 如果语句使用了 自增列（auto increment），或者依赖于数据库中的现有数据（例如，UPDATE ... WHERE <某些条件>），则必须在每个副本上按照完全相同的顺序执行它们，否则可能会产生不同的效果。当有多个并发执行的事务时，这可能成为一个限制。
- 有副作用的语句（例如：触发器、存储过程、用户定义的函数）可能会在每个副本上产生不同的副作用，除非副作用是绝对确定性的


基于传输预写式日志（WAL）
- 其主要缺点是日志记录的数据非常底层：WAL 包含哪些磁盘块中的哪些字节发生了更改。这使复制与存储引擎紧密耦合。如果数据库将其存储格式从一个版本更改为另一个版本，通常不可能在主库和从库上运行不同版本的数据库软件

逻辑日志复制 (基于行)
- 逻辑日志和存储引擎的内部实现是解耦的，系统可以更容易地做到向后兼容，从而使主库和从库能够运行不同版本的数据库软件，或者甚至不同的存储引擎

基于触发器的复制


2. 复制延迟问题

- 读己之写
写后读 (read-after-write) 的一致性来防止这种异常， 写后读一致性（read-after-write consistency）

- 单调读
- 一致前缀读


3. 多主复制

- 常见在多数据中心场景
- 给每个写入一个唯一的 ID（例如时间戳、长随机数、UUID 或者键和值的哈希），挑选最高 ID 的写入作为胜利者，并丢弃其他写入。
  如果使用时间戳，这种技术被称为 最后写入胜利（LWW, last write wins）


4. 无主复制


最后写入胜利（丢弃并发写入）
- 在数据库中使用 LWW 的唯一安全方法是确保一个键只写入一次，然后视为不可变，从而避免对同一个键进行并发更新。例如，Cassandra 推荐使用的方法是使用 UUID 作为键，从而为每个写操作提供一个唯一的键

==============================================================================================================================

